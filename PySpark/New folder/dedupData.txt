from pyspark.sql.window import Window
import pyspark.sql.functions as f

def dedupData(df, primary_keys, dedupe_columns, composite=False):
    """
  This function finds the most recent data indicated by a given ingestion date/timestamp/job id column for a given list of ID columns.
  
  Args:
    df              (spark dataframe): A spark dataframe
    primary_keys        (list of str): Primary key columns by which to partition
    dedupe_columns      (list of str): Columns indicating the ingestion date/timestamp/job id for the data
    
  Returns:
    deduped_df      (spark dataframe): The df deduplicated
  """
  # Create a composite key column that concats individual columns.
    if composite == True:
        df = (df.withColumn('COMPOSITE_KEY',f.concat(*primary_keys)))
        primary_keys = 'COMPOSITE_KEY'
  
  # Create window
    window_dedup = (
        Window
        .partitionBy(*primary_keys)
        .orderBy(*(f.col(dedupe_columns).desc() for dedupe_columns in dedupe_columns)))
  
    deduped_df = (
        df
        .withColumn("DATA_PROCESS_RANK", f.row_number().over(window_dedup))
        .filter("DATA_PROCESS_RANK = 1")
        .drop("DATA_PROCESS_RANK")
        .distinct())
  
    if composite == True:
        deduped_df.drop('COMPOSITE_KEY')
    
    return deduped_df






invoice_ex= dedupData(invoicemaster, ['INVOICEID','PATIENTID','CLIENTID','LINEITEM'],['EXTRACTION_TIMESTAMP'], composite=False).select('INVOICEID','PATIENTID','CLIENTID','LINEITEM')
invoice_ex.createOrReplaceTempView("invoice_ex")
#invoice_ex.display()